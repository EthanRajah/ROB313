{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Assignment 2 ROB313**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import revelant libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from data.data_utils import load_dataset\n",
    "from sklearn.neighbors import KDTree\n",
    "from scipy.linalg import cho_factor, cho_solve\n",
    "np.random.seed(21)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Question 3 - Creating a Radial Basis Function (RBF)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.14812442755080485, 2.0, 0.001)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gaussian_kernel(x_train, x_test, theta):\n",
    "    # reshape the matricies correctly for broadcasting\n",
    "    x = np.expand_dims(x_train, axis=1)\n",
    "    z = np.expand_dims(x_test, axis=0)\n",
    "    # now evaluate the kernel using the euclidean distances squared between points\n",
    "    return np.exp(-np.sum(np.square(x-z)/theta, axis=2, keepdims=False))\n",
    "\n",
    "def rbf_model(x_train, y_train, x_valid, y_valid):\n",
    "    '''Format data, determine alpha by calculating gram matrix and usng Cholesky factorization, apply RBF model, iterate over values of theta and lamda and store RMSE info'''\n",
    "    thet = np.array([0.05, 0.1, 0.5, 1, 2])\n",
    "    lam = np.array([0.001, 0.01, 0.1, 1])\n",
    "    rmse = np.zeros((thet.shape[0], lam.shape[0]))\n",
    "    # Select hyperparameters by evaluating on the validation set\n",
    "    for i,theta in enumerate(thet):\n",
    "        for j,lamda in enumerate(lam):\n",
    "            K = gaussian_kernel(x_train, x_train, theta)\n",
    "            C = cho_factor(K + lamda*np.identity(x_train.shape[0]))\n",
    "            alpha = cho_solve(C,y_train)\n",
    "            # Use x_train as the landmarks (basis vectors) for predicting on validation set\n",
    "            y_rbf = gaussian_kernel(x_valid, x_train, theta).dot(alpha)\n",
    "            rmse[i][j] = np.sqrt(np.mean(np.square(y_rbf-y_valid)))\n",
    "    # Determine hyperparameters with smallest error\n",
    "    min_ind = np.unravel_index(np.argmin(rmse, axis=None), rmse.shape)\n",
    "    theta_best = thet[min_ind[0]]\n",
    "    lamda_best = lam[min_ind[1]]\n",
    "\n",
    "    return theta_best, lamda_best\n",
    "\n",
    "def test_rbf(x_train, y_train, x_valid, y_valid, x_test, y_test, theta, lamda):\n",
    "    # Use training and validation sets to predict on test set\n",
    "    x_train = np.vstack([x_train, x_valid])\n",
    "    y_train = np.vstack([y_train, y_valid])\n",
    "\n",
    "    # Calculate RBF using hyperparameters given\n",
    "    K = gaussian_kernel(x_train, x_train, theta)\n",
    "    C = cho_factor(K + lamda*np.identity(x_train.shape[0]))\n",
    "    alpha = cho_solve(C,y_train)\n",
    "    # Use x_train as the landmarks (basis vectors) for predicting on test set\n",
    "    y_rbf = gaussian_kernel(x_test, x_train, theta).dot(alpha)\n",
    "    rmse = np.sqrt(np.mean(np.square(y_rbf-y_test)))\n",
    "\n",
    "    return rmse\n",
    "    \n",
    "def run_Q3(dataset):\n",
    "    if dataset == 'mauna_loa':\n",
    "        x_train, x_valid, x_test, y_train, y_valid, y_test = load_dataset('mauna_loa')\n",
    "    elif dataset == 'rosenbrock':\n",
    "        x_train, x_valid, x_test, y_train, y_valid, y_test = load_dataset('rosenbrock', n_train=1000, d=2)\n",
    "\n",
    "    # Determine hyperparameters (regularization and shape) using training and validation sets\n",
    "    theta, lamda = rbf_model(x_train, y_train, x_valid, y_valid)\n",
    "    # Assess hyperparameter choices against test data\n",
    "    test_rmse = test_rbf(x_train, y_train, x_valid, y_valid, x_test, y_test, theta, lamda)\n",
    "\n",
    "    return test_rmse, theta, lamda\n",
    "\n",
    "run_Q3('rosenbrock')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results:**\n",
    "\n",
    "Mauna Loa:\n",
    "- Test RMSE: 0.14977338771865478\n",
    "- Theta: 1.0\n",
    "- Lamda = 0.001\n",
    "\n",
    "Rosenbrock:\n",
    "- Test RMSE: 0.14812442755080485\n",
    "- Theta: 2.0\n",
    "- Lamda: 0.001"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Question 4 - Implementing Greedy Regression Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ortho_match_pursuit(x_train, x_valid, x_test, y_train, y_valid, y_test):\n",
    "    '''Create basis dictionary, use algorithm to determine which basis functions minimize the loss, then use those basis functions and weights from algo as our final sparse model to predict on'''\n",
    "    pass\n",
    "\n",
    "def run_Q4():\n",
    "    x_train, x_valid, x_test, y_train, y_valid, y_test = load_dataset('mauna_loa')\n",
    "    ortho_match_pursuit(x_train, x_valid, x_test, y_train, y_valid, y_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff4f85d6e04298634172ac5d8264e7e9b556b95639fe52ebb9425c4d4cba0c9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
