{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Assignment 4 ROB313 - Neural Networks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import autograd.numpy as np\n",
    "from autograd.scipy.special import logsumexp\n",
    "import autograd.numpy.random as npr\n",
    "from autograd import grad\n",
    "from autograd.misc.optimizers import adam\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# path to data\n",
    "sys.path.append(os.path.join(os.getcwd(), \"../../../data\"))\n",
    "from data.data_utils import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuron Initialization and Activations\n",
    "\n",
    "def init_randn(m, n, rs=npr.RandomState(0)):\n",
    "    \"\"\" init mxn matrix using small random normal\"\"\"\n",
    "    return 0.1 * rs.randn(m, n)\n",
    "\n",
    "def init_xavier(m, n, rs=npr.RandomState(0)):\n",
    "    \"\"\"Init mxn matrix using Xavier intialization - initialization of weights\"\"\"\n",
    "    return rs.randn(m,n)/np.sqrt(n)\n",
    "\n",
    "def init_net_params(layer_sizes, init_fcn, rs=npr.RandomState(0)):\n",
    "    \"\"\" inits a (weights, biases) tuples for all layers using the intialize function (init_fcn)\"\"\"\n",
    "    return [\n",
    "        (init_fcn(m, n), np.zeros(n))  # weight matrix  # bias vector\n",
    "        for m, n in zip(layer_sizes[:-1], layer_sizes[1:])\n",
    "    ]\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network training and prediction\n",
    "\n",
    "def neural_net_predict(params, inputs):\n",
    "    \"\"\"Implements a deep neural network for classification.\n",
    "       params is a list of (weights, bias) tuples.\n",
    "       inputs is an (N x D) matrix.\n",
    "       returns normalized class log-probabilities.\"\"\"\n",
    "    for W, b in params:\n",
    "        outputs = np.dot(inputs, W) + b\n",
    "        inputs = relu(outputs)\n",
    "    return outputs - logsumexp(outputs, axis=1, keepdims=True)\n",
    "\n",
    "def mean_log_like(params, inputs, targets):\n",
    "    \"\"\" TODO: return the log-likelihood / the number of inputs \n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"mean_log_like function not completed (see Q3).\")\n",
    "\n",
    "def accuracy(params, inputs, targets):\n",
    "    \"\"\" return the accuracy of the neural network defined by params\n",
    "    \"\"\"\n",
    "    target_class = np.argmax(targets, axis=1)\n",
    "    predicted_class = np.argmax(neural_net_predict(params, inputs), axis=1)\n",
    "    return np.mean(predicted_class == target_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Code Usage\n",
    "\n",
    "# loading data\n",
    "x_train, x_valid, x_test, y_train, y_valid, y_test = load_dataset(\"mnist_small\")\n",
    "\n",
    "# initializing parameters\n",
    "layer_sizes = [784, 200, 10]\n",
    "params = init_net_params(layer_sizes, init_randn)\n",
    "\n",
    "# setting up training parameters\n",
    "num_epochs = 5\n",
    "learning_rate = 1e-1\n",
    "batch_size = 256\n",
    "\n",
    "# Constants for batching\n",
    "num_batches = int(np.ceil(len(x_train) / batch_size))\n",
    "rind = np.arange(len(x_train))\n",
    "npr.shuffle(rind)\n",
    "\n",
    "def batch_indices(iter):\n",
    "    idx = iter % num_batches\n",
    "    return rind[slice(idx * batch_size, (idx + 1) * batch_size)]\n",
    "\n",
    "# Define training objective\n",
    "def objective(params, iter):\n",
    "    # get indices of data in batch\n",
    "    idx = batch_indices(iter)\n",
    "    return -mean_log_like(params, x_train[idx], y_train[idx])\n",
    "\n",
    "# Get gradient of objective using autograd.\n",
    "objective_grad = grad(objective)\n",
    "\n",
    "print(\"     Epoch     |    Train accuracy  |       Test accuracy  \")\n",
    "\n",
    "# Dictionary to store train/val history\n",
    "opt_history = {\n",
    "    \"train_nll\": [],\n",
    "    \"val_nll\": [],\n",
    "}\n",
    "\n",
    "def callback(params, iter, gradient):\n",
    "    if iter % num_batches == 0:\n",
    "        # record training & val. accuracy every epoch\n",
    "        opt_history[\"train_nll\"].append(-mean_log_like(params, x_train, y_train))\n",
    "        opt_history[\"val_nll\"].append(-mean_log_like(params, x_valid, y_valid))\n",
    "        train_acc = accuracy(params, x_train, y_train)\n",
    "        val_acc = accuracy(params, x_valid, y_valid)\n",
    "        print(\"{:15}|{:20}|{:20}\".format(iter // num_batches, train_acc, val_acc))\n",
    "\n",
    "# We will optimize using Adam (a variant of SGD that makes better use of\n",
    "# gradient information).\n",
    "opt_params = adam(\n",
    "    objective_grad,\n",
    "    params,\n",
    "    step_size=learning_rate,\n",
    "    num_iters=num_epochs * num_batches,\n",
    "    callback=callback,\n",
    ")\n",
    "\n",
    "# Plotting the train and validation negative log-likelihood\n",
    "plt.plot(opt_history[\"train_nll\"])\n",
    "plt.plot(opt_history[\"val_nll\"])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "- Epsilon, the variance of the linear activation function must be equal to 1/sqrt(D^2)\n",
    "- In this case, D is the number of inputs to a neuron\n",
    "\n",
    "**Question 2**\n",
    "- The logsumexp function from scipy computes the log of the sum of exponentials of input elements for us. We have used the logsumexp function instead of a more naive approach (manual computation) to get the predicted log probabilities for each class because it does this computation in a more stable way. This function can help us prevent underflow/overflow errors, which is one issue that we experienced within assignment 3 when working with log likelihoods. For instance, the naive approach could crash in multiple instances due to division by 0 or when dealing with inf/nan values, thus causing numerical instability. Overall, the logsumexp function prevents this issue from occuring, therefore making our program more robust."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
